{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/emanueltay/Optimal_Subtitle_Placement/blob/main/caption_placement_preposition_testing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip /content/optimal_subtitle_files.zip -d /content/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tqdsIMOQ4XH",
        "outputId": "9bf5b080-36ff-46b5-bd8b-a61013531bff"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/optimal_subtitle_files.zip\n",
            "replace /content/optimal_subtitle/Font/OpenSans-Italic-VariableFont_wdth,wght.ttf? [y]es, [n]o, [A]ll, [N]one, [r]ename: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LG33s7ssRHhK",
        "outputId": "c3e75d6f-4c18-4593-97d8-be56e0bcd911"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.11/dist-packages (8.3.89)\n",
            "Requirement already satisfied: numpy<=2.1.1,>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.14.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.5.1+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.20.1+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.13.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.14)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.1.31)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.17.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2024.10.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Requirement already satisfied: triton==3.1.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJUwKTrUQgiY"
      },
      "source": [
        "### Detect Key Objects in the Frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "C5j6zUvEQgib"
      },
      "outputs": [],
      "source": [
        "# ! pip install ultralytics\n",
        "\n",
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import torch\n",
        "\n",
        "# Load the trained model\n",
        "model = YOLO(\"/content/optimal_subtitle/best.pt\")\n",
        "\n",
        "# ✅ Automatically select GPU if available\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "# ✅ Move model to the selected device\n",
        "model.to(device).float()  # Use FP32 (FP16 can cause issues on CPU)\n",
        "\n",
        "# ✅ Optimize PyTorch settings\n",
        "torch.backends.cudnn.benchmark = True  # Optimize for fixed-size inputs\n",
        "torch.backends.cudnn.enabled = True\n",
        "torch.set_num_threads(torch.get_num_threads())  # Use optimal number of CPU threads\n",
        "\n",
        "\n",
        "def detect_objects(frames):\n",
        "    \"\"\"\n",
        "    Perform batch object detection on multiple frames.\n",
        "\n",
        "    Parameters:\n",
        "        frames (list): List of frames (each a NumPy array).\n",
        "\n",
        "    Returns:\n",
        "        list: A list containing detections for each frame.\n",
        "              Each element is a NumPy array of detections.\n",
        "    \"\"\"\n",
        "    # ✅ Run the model in batch mode\n",
        "    results = model(frames, batch=len(frames))  # Run batch inference\n",
        "\n",
        "    # ✅ Extract detections for each frame\n",
        "    batch_detections = []\n",
        "    for result in results:\n",
        "        detections = result.boxes.data.cpu().numpy()  # Convert detections to NumPy array\n",
        "        batch_detections.append(detections)\n",
        "\n",
        "    return batch_detections  # List of detections per frame"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_eEdf6AYQgie"
      },
      "source": [
        "### Define Safe Zones for Subtitle Placement"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eR-MPZaZQgie"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import textwrap\n",
        "\n",
        "def calculate_safe_zone_with_prepositions_test(frame_width, frame_height, detections, pre_positions, subtitle_height=30, margin=10, shift_x=20):\n",
        "    \"\"\"\n",
        "    Calculate the safe zone for subtitle placement using pre-defined positions.\n",
        "    If blocked, it attempts to shift left/right before moving vertically.\n",
        "\n",
        "    Returns:\n",
        "        tuple: Coordinates of the safe zone (x1, y1, x2, y2).\n",
        "    \"\"\"\n",
        "\n",
        "    def zones_overlap(zone1, zone2):\n",
        "        \"\"\"Checks if two zones overlap.\"\"\"\n",
        "        x1a, y1a, x2a, y2a = zone1\n",
        "        x1b, y1b, x2b, y2b = zone2\n",
        "        return not (x2a < x1b or x1a > x2b or y2a < y1b or y1a > y2b)\n",
        "\n",
        "    # Step 1: Try Predefined Positions\n",
        "    for position_name, position in sorted(pre_positions.items(), key=lambda x: x[1].get(\"priority\", 0), reverse=True):\n",
        "        x1, y1, x2, y2 = position[\"coordinates\"]\n",
        "        # print(position[\"coordinates\"])\n",
        "        print(f\"Checking {position_name}...\")\n",
        "\n",
        "        # Check if the original pre-position is available\n",
        "        if not any(zones_overlap((x1, y1, x2, y2), detection[:4]) for detection in detections):\n",
        "            print(f\"✅ Using original {position_name}\")\n",
        "            return (x1, y1, x2, y2)  # Return if it's available\n",
        "\n",
        "        # min_width = 850  # Minimum width to prevent collapse\n",
        "        min_width = max(0.6 * frame_width, 600)  # 60% of frame width, but at least 600px\n",
        "        for shift_dir in [\"left\", \"right\"]:\n",
        "            shift_attempts = 0\n",
        "            while shift_attempts < 10:  # Try shifting multiple times\n",
        "                if shift_dir == \"left\":\n",
        "                    new_x1, new_x2 = max(x1*10, x1 - shift_x), max(min_width, x2 - shift_x)\n",
        "                else:\n",
        "                    new_x1, new_x2 = min(frame_width - min_width, x1 + shift_x), min(frame_width, x2 + shift_x)\n",
        "\n",
        "                shifted_zone = (new_x1, y1, new_x2, y2)\n",
        "\n",
        "                if new_x2 > new_x1 and not any(zones_overlap(shifted_zone, detection[:4]) for detection in detections):\n",
        "                    print(f\"✅ Shifted {position_name} {shift_dir} and found a free spot.\")\n",
        "                    return shifted_zone  # Found a valid shifted zone\n",
        "\n",
        "                shift_attempts += 1\n",
        "                shift_x *= 1.5  # Increase shift size if first shift attempt fails\n",
        "\n",
        "    # Step 2: Fallback to Dynamic Safe Zone Calculation (Starting from Bottom)\n",
        "    print(\"⚠ No predefined positions worked. Trying dynamic safe zone...\")\n",
        "    proposed_safe_zone = (0, frame_height - subtitle_height - margin, frame_width, frame_height - margin)\n",
        "\n",
        "    while True:\n",
        "        if all(not zones_overlap(proposed_safe_zone, (int(d[0]), int(d[1]), int(d[2]), int(d[3]))) for d in detections):\n",
        "            print(\"✅ Dynamic safe zone found.\")\n",
        "            return proposed_safe_zone  # Found a safe area\n",
        "\n",
        "        # Try shifting up\n",
        "        x1, y1, x2, y2 = proposed_safe_zone\n",
        "        new_y1 = y1 - subtitle_height - margin\n",
        "        new_y2 = y2 - subtitle_height - margin\n",
        "\n",
        "        if new_y1 < 0:\n",
        "            break  # No valid space above, fallback required\n",
        "\n",
        "        proposed_safe_zone = (x1, new_y1, x2, new_y2)\n",
        "\n",
        "    # Step 3: Final fallback to the top of the frame\n",
        "    print(\"⚠ No valid spaces found, defaulting to top position.\")\n",
        "    return (0, margin, frame_width, subtitle_height + margin)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42kgNBk0Qgif"
      },
      "source": [
        "### Subtitle size and margin"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "8mVacuTdQgif"
      },
      "outputs": [],
      "source": [
        "def get_subtitle_size(frame_height):\n",
        "    \"\"\"\n",
        "    Dynamically calculate subtitle height and margin based on frame resolution.\n",
        "\n",
        "    Parameters:\n",
        "        frame_height (int): Height of the video frame.\n",
        "\n",
        "    Returns:\n",
        "        tuple: (subtitle_height, margin)\n",
        "    \"\"\"\n",
        "    subtitle_height = max(0.05 * frame_height, 18)  # Minimum 18px for readability\n",
        "    margin = max(0.02 * frame_height, 5)  # Minimum 5px to avoid text touching edges\n",
        "\n",
        "    return int(subtitle_height), int(margin)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EvQkMeKQgig"
      },
      "source": [
        "### Subtitle character calculation"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install arabic-reshaper"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yc53x_yQR8hq",
        "outputId": "18f02ec3-686e-4f80-95ef-055aa5a49cf6"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: arabic-reshaper in /usr/local/lib/python3.11/dist-packages (3.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install python-bidi"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_ndbnMMTJqr",
        "outputId": "400ee6c0-3f90-4706-a2be-1391cf81b553"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: python-bidi in /usr/local/lib/python3.11/dist-packages (0.6.6)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "ML2qsOboQgig"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import re\n",
        "import textwrap\n",
        "import numpy as np\n",
        "import arabic_reshaper\n",
        "from bidi.algorithm import get_display\n",
        "from PIL import ImageFont, ImageDraw, Image\n",
        "\n",
        "## universal font that fits all languages\n",
        "def get_font(font_size):\n",
        "\n",
        "    \"\"\"\n",
        "    Loads the universal OpenSans font.\n",
        "\n",
        "    Parameters:\n",
        "        font_size (int): The desired font size.\n",
        "\n",
        "    Returns:\n",
        "        PIL.ImageFont: The loaded font.\n",
        "    \"\"\"\n",
        "    font_path = \"/content/optimal_subtitle/Font/GoNotoKurrent-Regular.ttf\"\n",
        "    return ImageFont.truetype(font_path, font_size)\n",
        "\n",
        "# **2️⃣ Function to Detect Language Type**\n",
        "def detect_language(text):\n",
        "    \"\"\"\n",
        "    Detects if the text contains Latin, CJK, Arabic, or Indic characters.\n",
        "\n",
        "    Returns:\n",
        "        str: \"latin\", \"cjk\", \"arabic\", \"indic\", \"thai\" based on detected script.\n",
        "    \"\"\"\n",
        "\n",
        "    # Extract the first two words\n",
        "    words = re.findall(r'\\b\\w+\\b', text)  # Split text into words\n",
        "    text_snippet = \" \".join(words[:2])  # Take only the first two words\n",
        "\n",
        "    if any(\"\\u0600\" <= ch <= \"\\u06FF\" for ch in text_snippet):  # Arabic script range\n",
        "        return \"arabic\"\n",
        "    elif any(\"\\u4E00\" <= ch <= \"\\u9FFF\" for ch in text_snippet):  # Chinese script range\n",
        "        return \"cjk\"\n",
        "    elif any(\"\\u3040\" <= ch <= \"\\u30FF\" for ch in text_snippet):  # Japanese script range\n",
        "        return \"cjk\"\n",
        "    elif any(\"\\uAC00\" <= ch <= \"\\uD7AF\" for ch in text_snippet):  # Korean script range\n",
        "        return \"cjk\"\n",
        "    elif any(\"\\u0900\" <= ch <= \"\\u097F\" for ch in text_snippet):  # Devanagari script (Hindi, Marathi, Sanskrit)\n",
        "        return \"indic\"\n",
        "    elif any(\"\\u0E00\" <= ch <= \"\\u0E7F\" for ch in text_snippet):  # Thai script\n",
        "        return \"thai\"\n",
        "    return \"latin\"  # Default to Latin if nothing is detected\n",
        "\n",
        "# **3️⃣ Main Subtitle Rendering Function**\n",
        "def render_subtitle_multi_new(frame, subtitle_text, safe_zone, frame_width, frame_height, max_chars_per_line=40, opacity=0.8):\n",
        "    \"\"\"\n",
        "    Render multi-line subtitles centered within the safe zone with a semi-transparent background.\n",
        "\n",
        "    Parameters:\n",
        "        frame (numpy array): The frame on which to render subtitles.\n",
        "        subtitle_text (str): The text to display.\n",
        "        safe_zone (tuple): (x1, y1, x2, y2) defining subtitle placement.\n",
        "        frame_width (int): Width of the frame.\n",
        "        frame_height (int): Height of the frame.\n",
        "        opacity (float): Background opacity (0 = fully transparent, 1 = fully opaque).\n",
        "\n",
        "    Returns:\n",
        "        numpy array: The frame with subtitles rendered at an optimal position.\n",
        "    \"\"\"\n",
        "    x1, y1, x2, y2 = safe_zone\n",
        "    language = detect_language(subtitle_text)  # **Detect language**\n",
        "    font_size = 28 if language == \"cjk\" else 26  # Adjust font size for CJK characters\n",
        "    # font = get_font_for_language(language, font_size)  # Load correct font\n",
        "    font = get_font(font_size)\n",
        "\n",
        "    # **Handle Right-to-Left (RTL) text (e.g., Arabic)**\n",
        "    if language == \"arabic\":\n",
        "        subtitle_text = get_display(arabic_reshaper.reshape(subtitle_text))\n",
        "\n",
        "    # **Calculate max width available for text**\n",
        "    max_text_width = x2 - x1 - 30  # Ensure padding (30)\n",
        "    # print(\"Max width in pixels:\", max_text_width)\n",
        "\n",
        "    # **Estimate Average Character Width Dynamically Using Subtitle Text**\n",
        "    if len(subtitle_text) > 0:\n",
        "        char_width = sum(font.getbbox(char)[2] - font.getbbox(char)[0] for char in subtitle_text) / len(subtitle_text)\n",
        "    else:\n",
        "        char_width = font_size // 2  # Fallback for empty text\n",
        "\n",
        "    # **Determine Maximum Characters That Fit in Safe Zone**\n",
        "    estimated_max_chars = max_text_width // char_width\n",
        "    # print(\"Estimated max characters per line:\", int(estimated_max_chars))\n",
        "\n",
        "    # **Use the Minimum of User-Defined or Estimated Max Chars**\n",
        "    final_max_chars_per_line = min(estimated_max_chars, max_chars_per_line)\n",
        "\n",
        "    # **Dynamically wrap text based on max character limit**\n",
        "    wrapped_lines = []\n",
        "    for line in subtitle_text.split(\"\\n\"):  # Handle existing line breaks\n",
        "        new_lines = textwrap.wrap(line, width=int(final_max_chars_per_line))\n",
        "        if new_lines:  # Only extend if wrapping produced text\n",
        "            wrapped_lines.extend(new_lines)\n",
        "\n",
        "    # **Fallback to prevent empty wrapped_lines**\n",
        "    if not wrapped_lines:\n",
        "        wrapped_lines = [\" \"]  # Ensures at least one blank line\n",
        "\n",
        "    # **Measure Text Size**\n",
        "    text_sizes = [font.getbbox(line) for line in wrapped_lines]\n",
        "    text_width = max(size[2] - size[0] for size in text_sizes)  # Width (right - left)\n",
        "    text_height = text_sizes[0][3] - text_sizes[0][1]  # Height (bottom - top)\n",
        "    total_text_height = sum(size[3] - size[1] for size in text_sizes) + (len(wrapped_lines) - 1) * 10  # Extra spacing\n",
        "\n",
        "    # **Center Text Within Safe Zone**\n",
        "    text_x = x1 + (x2 - x1 - text_width) // 2  # **Horizontally centered**\n",
        "    text_y = y1 + (y2 - y1 - total_text_height) // 2 - 20# **Vertically centered**\n",
        "\n",
        "    # **Define Background Box**\n",
        "    bg_x1 = max(text_x - 15, 0)\n",
        "    bg_y1 = max(text_y - 5, 0)\n",
        "    bg_x2 = min(text_x + text_width + 15, frame_width - 1)\n",
        "    bg_y2 = min(text_y + total_text_height + 15, frame_height - 1)\n",
        "\n",
        "    # **Create Semi-Transparent Background**\n",
        "    overlay = frame.copy()\n",
        "    cv2.rectangle(overlay, (bg_x1, bg_y1), (bg_x2, bg_y2), (0, 0, 0), -1)  # Black background\n",
        "    cv2.addWeighted(overlay, opacity, frame, 1 - opacity, 0, frame)  # Blend overlay with frame\n",
        "\n",
        "    # **Render Text Using PIL (for better font handling)**\n",
        "    frame_pil = Image.fromarray(frame)\n",
        "    draw = ImageDraw.Draw(frame_pil)\n",
        "\n",
        "    y_offset = text_y\n",
        "    for line in wrapped_lines:\n",
        "        line_width = font.getbbox(line)[2] - font.getbbox(line)[0]  # Measure width\n",
        "        line_x = x1 + (x2 - x1 - line_width) // 2  # Center per line\n",
        "        draw.text((line_x, y_offset), line, font=font, fill=(255, 255, 255))  # White text\n",
        "        y_offset += text_height + 10  # Extra line spacing\n",
        "\n",
        "    return np.array(frame_pil)  # Convert back to OpenCV format"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7LgkgGkQgij"
      },
      "source": [
        "### Complete Pipeline for frames batch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "27_UIalCQgij"
      },
      "outputs": [],
      "source": [
        "import json\n",
        "\n",
        "def process_frames_batch(frames, subtitles):\n",
        "    \"\"\"\n",
        "    Process a batch of frames:\n",
        "    - Detects objects in batch\n",
        "    - Computes safe zones in batch\n",
        "    - Overlays subtitles in batch\n",
        "\n",
        "    Parameters:\n",
        "        frames (list): List of frames (NumPy arrays).\n",
        "        subtitles (list): List of subtitles corresponding to each frame.\n",
        "\n",
        "    Returns:\n",
        "        list: Processed frames with subtitles.\n",
        "    \"\"\"\n",
        "\n",
        "    # Step 1: ✅ Batch Detect Objects (Faster than per-frame)\n",
        "    batch_detections = detect_objects(frames)  # Runs model on all frames at once\n",
        "\n",
        "    processed_frames = []\n",
        "    frame_height, frame_width = frames[0].shape[:2]\n",
        "\n",
        "    # Load precomputed safe zone positions (JSON file only loaded once)\n",
        "    with open(\"/content/optimal_subtitle/news_video_subtitle_positions.json\", \"r\") as file:\n",
        "        pre_positions = json.load(file)[f\"{frame_width}x{frame_height}\"]\n",
        "\n",
        "    # Step 2: ✅ Process Each Frame in the Batch\n",
        "    for i, frame in enumerate(frames):\n",
        "        subtitle_text = subtitles[i]  # Get corresponding subtitle\n",
        "        detections = batch_detections[i]  # Get detections for the frame\n",
        "\n",
        "        # Compute subtitle safe zone for the frame\n",
        "        subtitle_height, margin = get_subtitle_size(frame_height)\n",
        "        safe_zone = calculate_safe_zone_with_prepositions_test(\n",
        "            frame_width,\n",
        "            frame_height,\n",
        "            detections,\n",
        "            pre_positions,\n",
        "            subtitle_height,\n",
        "            margin\n",
        "        )\n",
        "        safe_zone = tuple(map(int, safe_zone))  # Convert values to integers\n",
        "\n",
        "        # Render subtitle and save processed frame\n",
        "        processed_frame = render_subtitle_multi_new(frame, subtitle_text, safe_zone, frame_width, frame_height)\n",
        "        processed_frames.append(processed_frame)\n",
        "\n",
        "    return processed_frames  # Return list of processed frames\n",
        "    # return None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDwa2goMQgik"
      },
      "source": [
        "## Testing the Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoWr2HYHQgik"
      },
      "source": [
        "### Integrate with srt file and video fps"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pysrt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc0SuEOLTtPo",
        "outputId": "d0086c9c-ff0a-49bd-df36-57c45b158b01"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pysrt in /usr/local/lib/python3.11/dist-packages (1.1.2)\n",
            "Requirement already satisfied: chardet in /usr/local/lib/python3.11/dist-packages (from pysrt) (5.2.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HNDyfzysQgik"
      },
      "outputs": [],
      "source": [
        "import pysrt\n",
        "from collections import defaultdict\n",
        "\n",
        "def parse_srt_file(srt_file):\n",
        "    \"\"\"\n",
        "    Reads and parses an SRT file, pre-indexing subtitles for fast lookup.\n",
        "\n",
        "    Parameters:\n",
        "        srt_file (str): Path to the .srt file.\n",
        "\n",
        "    Returns:\n",
        "        dict: A dictionary where keys are integer timestamps (seconds),\n",
        "              and values are subtitle texts.\n",
        "    \"\"\"\n",
        "    subs = pysrt.open(srt_file)\n",
        "    subtitle_dict = defaultdict(lambda: None)  # Default to None for missing frames\n",
        "\n",
        "    for sub in subs:\n",
        "        start_time = int(sub.start.minutes * 60 + sub.start.seconds)  # Round to nearest second\n",
        "        end_time = int(sub.end.minutes * 60 + sub.end.seconds)\n",
        "        subtitle_text = sub.text.replace(\"\\n\", \" \")  # Convert newlines to spaces\n",
        "\n",
        "        # ✅ Store subtitles for all frames in the time range\n",
        "        for t in range(start_time, end_time + 1):\n",
        "            subtitle_dict[t] = subtitle_text\n",
        "\n",
        "    return subtitle_dict  # Faster lookups using a dictionary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "ifw8RmT7Qgil"
      },
      "outputs": [],
      "source": [
        "def get_subtitles_for_frames(frame_times, subtitle_dict):\n",
        "    \"\"\"\n",
        "    Retrieves subtitles for a batch of frame timestamps.\n",
        "\n",
        "    Parameters:\n",
        "        frame_times (list): List of timestamps (in seconds).\n",
        "        subtitle_dict (dict): Pre-indexed subtitle dictionary.\n",
        "\n",
        "    Returns:\n",
        "        list: List of subtitle texts corresponding to each frame timestamp.\n",
        "    \"\"\"\n",
        "    return [subtitle_dict.get(int(time), \"\") for time in frame_times]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_video_fps(video_path):\n",
        "    \"\"\"Extracts FPS from a video using FFmpeg.\"\"\"\n",
        "    cmd = [\"ffmpeg\", \"-i\", video_path]\n",
        "\n",
        "    # ✅ Use stdout and stderr explicitly\n",
        "    result = subprocess.run(cmd, stdout=subprocess.PIPE, stderr=subprocess.PIPE, text=True)\n",
        "\n",
        "    # ✅ Parse FPS from FFmpeg output\n",
        "    for line in result.stderr.split(\"\\n\"):\n",
        "        if \"Stream\" in line and \"Video\" in line and \"fps\" in line:\n",
        "            fps_value = float(line.split(\"fps\")[0].strip().split()[-1])  # Extract FPS\n",
        "            return fps_value\n",
        "\n",
        "    return 30  # Default to 30 FPS if not found"
      ],
      "metadata": {
        "id": "bDPjN9-7b5sS"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## video latency fixed\n",
        "\n",
        "import os\n",
        "import cv2\n",
        "import pysrt\n",
        "import torch\n",
        "import subprocess\n",
        "\n",
        "# ✅ Define file paths in /content/\n",
        "video_input_path = \"/content/optimal_subtitle/test_video_4.mp4\"\n",
        "final_video_path = \"/content/optimal_subtitle/test_video_with_audio.mp4\"\n",
        "srt_file_path = \"/content/optimal_subtitle/SRT_file/test_video_4_eng.srt\"\n",
        "\n",
        "# ✅ Create temporary paths inside /content/\n",
        "audio_path = \"/content/temp_audio.aac\"\n",
        "output_video_path = \"/content/temp_video_no_audio.mp4\"\n",
        "\n",
        "# ✅ Extract FPS dynamically\n",
        "fps = get_video_fps(video_input_path)\n",
        "print(f\"✅ Corrected FPS: {fps}\")\n",
        "\n",
        "# ✅ Extract audio from the original video\n",
        "# os.system(f\"ffmpeg -i {video_input_path} -q:a 0 -map a {audio_path}\")\n",
        "\n",
        "# ✅ Load Pre-indexed Subtitles\n",
        "subtitle_data = parse_srt_file(srt_file_path)\n",
        "\n",
        "# ✅ Open Video File\n",
        "cap = cv2.VideoCapture(video_input_path)\n",
        "frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "\n",
        "# ✅ Initialize Video Writer\n",
        "fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, fps, (frame_width, frame_height))\n",
        "\n",
        "# ✅ Process Video with Batching\n",
        "batch_size = 32\n",
        "frame_buffer = []\n",
        "timestamp_buffer = []\n",
        "frame_number = 0\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break  # Exit if no more frames\n",
        "\n",
        "    frame_time = frame_number / fps  # Convert frame number to timestamp\n",
        "    frame_buffer.append(frame)\n",
        "    timestamp_buffer.append(frame_time)\n",
        "\n",
        "    # ✅ Process in batch when buffer reaches batch_size\n",
        "    if len(frame_buffer) == batch_size:\n",
        "        # subtitles = get_subtitles_for_frames(timestamp_buffer, subtitle_data)  # ✅ Batch subtitle lookup\n",
        "        # processed_frames = process_frames_batch(frame_buffer, subtitles)  # ✅ Batch processing\n",
        "        processed_frames = detect_objects(frame_buffer)\n",
        "\n",
        "#         for processed_frame in processed_frames:\n",
        "#             out.write(processed_frame)  # Write frame to temporary video file\n",
        "\n",
        "#         frame_buffer.clear()\n",
        "#         timestamp_buffer.clear()  # Reset batch buffers\n",
        "\n",
        "\n",
        "#     frame_number += 1\n",
        "\n",
        "# # ✅ Process remaining frames if they exist\n",
        "# if frame_buffer:\n",
        "#     subtitles = get_subtitles_for_frames(timestamp_buffer, subtitle_data)\n",
        "#     processed_frames = process_frames_batch(frame_buffer, subtitles)\n",
        "#     for processed_frame in processed_frames:\n",
        "#         out.write(processed_frame)\n",
        "\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "# print(f\"✅ Video without audio saved at: {output_video_path}\")\n",
        "\n",
        "# # ✅ Merge Video & Audio with Correct FPS & Sync Fixes\n",
        "# os.system(f\"ffmpeg -i {output_video_path} -i {audio_path} -map 0:v:0 -map 1:a:0 -c:v libx264 -preset fast -crf 23 -c:a copy -vsync vfr {final_video_path}\")\n",
        "\n",
        "# # ✅ Clean up the temporary files\n",
        "# os.remove(audio_path)\n",
        "# os.remove(output_video_path)\n",
        "\n",
        "# print(f\"✅ Final video with audio saved at: {final_video_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8LLtOeWUT-gh",
        "outputId": "fc5ae33b-9b98-4f2d-814d-cd1194be392b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Corrected FPS: 29.97\n",
            "\n",
            "0: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "1: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "2: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "3: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "4: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "5: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "6: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "7: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "8: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "9: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "10: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "11: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "12: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "13: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "14: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "15: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "16: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "17: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "18: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "19: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "20: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "21: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "22: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "23: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "24: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "25: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "26: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "27: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "28: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "29: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "30: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "31: 384x640 1 face, 1 logo, 1 news-ticker, 3 texts, 45.4ms\n",
            "Speed: 4.7ms preprocess, 45.4ms inference, 17.6ms postprocess per image at shape (32, 3, 384, 640)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4yMuKdSOdHbo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "42kgNBk0Qgif"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}